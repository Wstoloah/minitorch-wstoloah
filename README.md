# minitorch
The full minitorch student suite.


To access the autograder:

* Module 0: https://classroom.github.com/a/qDYKZff9
* Module 1: https://classroom.github.com/a/6TiImUiy
* Module 2: https://classroom.github.com/a/0ZHJeTA0
* Module 3: https://classroom.github.com/a/U5CMJec1
* Module 4: https://classroom.github.com/a/04QA6HZK
* Quizzes: https://classroom.github.com/a/bGcGc12k
# Training Results for Neural Network with Tensor Backend

- Hidden size: `10`
- Learning rate: `0.5`
- Epochs: `500`
- Dataset size: `250`

## Dataset: `Simple`
- Time per epoch: **0.0014** seconds
- Final loss: **5.0389**
- Final accuracy: **100.00%**

### Loss and Accuracy per Epoch

| Epoch | Loss | Accuracy (%) |
|-------|------|---------------|
| 1 | 101.2409 | 96.00 |
| 2 | 95.2339 | 82.40 |
| 3 | 87.5836 | 81.20 |
| 4 | 71.4405 | 85.20 |
| 5 | 43.1978 | 91.20 |
| 6 | 43.8974 | 90.00 |
| 7 | 35.9936 | 92.00 |
| 8 | 27.7540 | 93.60 |
| 9 | 29.1159 | 93.20 |
| 10 | 28.3477 | 93.60 |
| 11 | 23.5300 | 94.40 |
| 12 | 21.6213 | 94.80 |
| 13 | 22.6311 | 94.40 |
| 14 | 21.8848 | 94.40 |
| 15 | 19.5560 | 95.60 |
| 16 | 19.0224 | 95.60 |
| 17 | 18.4158 | 96.00 |
| 18 | 17.8331 | 96.40 |
| 19 | 17.9899 | 96.00 |
| 20 | 17.3079 | 96.40 |
| 21 | 16.8305 | 96.80 |
| 22 | 16.2114 | 96.80 |
| 23 | 15.3626 | 96.80 |
| 24 | 14.8387 | 96.80 |
| 25 | 14.7387 | 96.80 |
| 26 | 15.0177 | 96.80 |
| 27 | 15.1036 | 96.80 |
| 28 | 14.2969 | 96.80 |
| 29 | 13.7074 | 97.20 |
| 30 | 13.1976 | 97.20 |
| 31 | 13.1313 | 97.20 |
| 32 | 13.5637 | 97.20 |
| 33 | 13.6077 | 96.80 |
| 34 | 13.1321 | 97.20 |
| 35 | 12.4139 | 98.00 |
| 36 | 12.4340 | 97.60 |
| 37 | 12.9213 | 97.20 |
| 38 | 11.9014 | 98.00 |
| 39 | 12.3599 | 97.60 |
| 40 | 12.2702 | 98.00 |
| 41 | 10.6869 | 98.00 |
| 42 | 10.5578 | 98.00 |
| 43 | 11.6140 | 98.00 |
| 44 | 13.8138 | 96.80 |
| 45 | 13.5775 | 96.80 |
| 46 | 9.4973 | 98.00 |
| 47 | 6.8088 | 99.20 |
| 48 | 5.6459 | 99.60 |
| 49 | 5.2338 | 99.60 |
| 50 | 5.0389 | 100.00 |

## Dataset: `Diag`
- Time per epoch: **0.0014** seconds
- Final loss: **18.3603**
- Final accuracy: **96.80%**

### Loss and Accuracy per Epoch

| Epoch | Loss | Accuracy (%) |
|-------|------|---------------|
| 1 | 58.5087 | 89.60 |
| 2 | 42.8236 | 89.60 |
| 3 | 31.9342 | 94.40 |
| 4 | 25.1958 | 96.40 |
| 5 | 20.7603 | 96.80 |
| 6 | 17.7293 | 97.60 |
| 7 | 15.5672 | 98.40 |
| 8 | 13.9531 | 98.40 |
| 9 | 12.6993 | 98.40 |
| 10 | 11.6949 | 98.80 |
| 11 | 10.8711 | 98.80 |
| 12 | 10.1831 | 98.80 |
| 13 | 9.6008 | 98.80 |
| 14 | 9.0977 | 98.80 |
| 15 | 8.6573 | 98.80 |
| 16 | 8.2687 | 99.20 |
| 17 | 7.9227 | 99.20 |
| 18 | 7.6132 | 99.20 |
| 19 | 7.3339 | 99.60 |
| 20 | 7.0800 | 99.60 |
| 21 | 6.8479 | 99.60 |
| 22 | 6.6345 | 99.60 |
| 23 | 6.4376 | 99.60 |
| 24 | 6.2551 | 99.60 |
| 25 | 6.0856 | 99.60 |
| 26 | 5.9273 | 99.60 |
| 27 | 5.7790 | 99.60 |
| 28 | 5.6398 | 99.60 |
| 29 | 5.5088 | 99.60 |
| 30 | 5.3853 | 99.60 |
| 31 | 5.2685 | 99.60 |
| 32 | 5.1578 | 99.60 |
| 33 | 5.0530 | 99.60 |
| 34 | 4.9534 | 99.60 |
| 35 | 4.8586 | 99.60 |
| 36 | 4.7683 | 99.60 |
| 37 | 4.6820 | 99.60 |
| 38 | 4.5995 | 99.60 |
| 39 | 4.5206 | 99.60 |
| 40 | 4.4449 | 99.60 |
| 41 | 4.3723 | 99.60 |
| 42 | 4.3026 | 99.60 |
| 43 | 4.2356 | 99.60 |
| 44 | 4.1715 | 99.60 |
| 45 | 4.1108 | 99.60 |
| 46 | 4.0583 | 99.60 |
| 47 | 4.0453 | 99.60 |
| 48 | 4.2978 | 100.00 |
| 49 | 6.7741 | 98.00 |
| 50 | 18.3603 | 96.80 |

## Dataset: `Split`
- Time per epoch: **0.0013** seconds
- Final loss: **11.4504**
- Final accuracy: **100.00%**

### Loss and Accuracy per Epoch

| Epoch | Loss | Accuracy (%) |
|-------|------|---------------|
| 1 | 160.6357 | 65.60 |
| 2 | 153.7160 | 68.40 |
| 3 | 143.2559 | 77.60 |
| 4 | 129.4874 | 89.20 |
| 5 | 132.8400 | 74.00 |
| 6 | 103.4868 | 88.40 |
| 7 | 104.2663 | 82.40 |
| 8 | 99.6903 | 81.20 |
| 9 | 92.6222 | 81.60 |
| 10 | 88.4623 | 82.00 |
| 11 | 84.1900 | 82.40 |
| 12 | 79.4745 | 83.20 |
| 13 | 76.0229 | 83.20 |
| 14 | 72.0395 | 83.20 |
| 15 | 69.0556 | 84.40 |
| 16 | 65.7200 | 85.20 |
| 17 | 62.7399 | 85.60 |
| 18 | 59.9021 | 86.40 |
| 19 | 57.2774 | 86.80 |
| 20 | 54.8627 | 88.00 |
| 21 | 53.0839 | 88.00 |
| 22 | 50.9037 | 88.80 |
| 23 | 48.2505 | 89.60 |
| 24 | 47.3968 | 90.40 |
| 25 | 47.2322 | 90.40 |
| 26 | 43.1028 | 90.40 |
| 27 | 40.3463 | 91.20 |
| 28 | 43.5511 | 90.40 |
| 29 | 44.2757 | 90.40 |
| 30 | 35.7770 | 91.60 |
| 31 | 30.3179 | 93.60 |
| 32 | 36.8064 | 92.00 |
| 33 | 59.9951 | 87.20 |
| 34 | 31.5211 | 93.60 |
| 35 | 17.9065 | 98.00 |
| 36 | 15.7604 | 99.20 |
| 37 | 15.7721 | 98.40 |
| 38 | 34.9387 | 92.80 |
| 39 | 132.0774 | 80.00 |
| 40 | 16.8435 | 99.20 |
| 41 | 14.6671 | 99.60 |
| 42 | 13.6018 | 99.60 |
| 43 | 12.7861 | 99.60 |
| 44 | 12.1203 | 99.60 |
| 45 | 11.5982 | 100.00 |
| 46 | 12.2412 | 99.60 |
| 47 | 247.6003 | 77.60 |
| 48 | 15.7121 | 98.00 |
| 49 | 12.2527 | 99.60 |
| 50 | 11.4504 | 100.00 |

## Dataset: `Xor`
- Time per epoch: **0.0013** seconds
- Final loss: **20.4443**
- Final accuracy: **98.00%**

### Loss and Accuracy per Epoch

| Epoch | Loss | Accuracy (%) |
|-------|------|---------------|
| 1 | 162.9989 | 70.00 |
| 2 | 148.9858 | 70.40 |
| 3 | 136.0138 | 68.80 |
| 4 | 124.6304 | 71.20 |
| 5 | 114.0490 | 80.00 |
| 6 | 111.9512 | 72.00 |
| 7 | 109.5008 | 71.60 |
| 8 | 107.4439 | 74.00 |
| 9 | 104.3378 | 75.60 |
| 10 | 94.5039 | 80.40 |
| 11 | 91.2104 | 80.00 |
| 12 | 91.6886 | 80.00 |
| 13 | 86.9910 | 81.60 |
| 14 | 83.7182 | 82.80 |
| 15 | 81.3134 | 83.60 |
| 16 | 77.9897 | 84.80 |
| 17 | 77.2608 | 84.80 |
| 18 | 72.1056 | 86.40 |
| 19 | 66.9683 | 87.20 |
| 20 | 70.2856 | 86.40 |
| 21 | 58.1102 | 88.80 |
| 22 | 90.3888 | 82.00 |
| 23 | 44.9643 | 93.20 |
| 24 | 92.3938 | 81.60 |
| 25 | 41.1708 | 94.00 |
| 26 | 79.6664 | 85.20 |
| 27 | 40.8367 | 95.20 |
| 28 | 39.1588 | 93.20 |
| 29 | 102.4308 | 82.00 |
| 30 | 36.4740 | 94.80 |
| 31 | 34.2506 | 95.20 |
| 32 | 36.4387 | 93.60 |
| 33 | 52.2518 | 89.60 |
| 34 | 33.3828 | 96.40 |
| 35 | 30.9773 | 96.80 |
| 36 | 29.0570 | 97.60 |
| 37 | 30.2144 | 96.40 |
| 38 | 45.7018 | 91.20 |
| 39 | 30.2941 | 96.80 |
| 40 | 27.9589 | 97.60 |
| 41 | 26.1441 | 98.00 |
| 42 | 24.7222 | 98.00 |
| 43 | 24.8914 | 96.80 |
| 44 | 314.7446 | 69.60 |
| 45 | 28.5175 | 98.00 |
| 46 | 24.9662 | 98.00 |
| 47 | 23.2240 | 98.00 |
| 48 | 22.0768 | 98.00 |
| 49 | 21.1897 | 98.00 |
| 50 | 20.4443 | 98.00 |

## Dataset: `Circle`
- Time per epoch: **0.0011** seconds
- Final loss: **50.4984**
- Final accuracy: **90.80%**

### Loss and Accuracy per Epoch

| Epoch | Loss | Accuracy (%) |
|-------|------|---------------|
| 1 | 146.7614 | 70.00 |
| 2 | 138.8128 | 70.00 |
| 3 | 131.0835 | 70.00 |
| 4 | 123.3248 | 70.00 |
| 5 | 114.0836 | 76.80 |
| 6 | 110.6255 | 83.60 |
| 7 | 124.1567 | 82.80 |
| 8 | 110.7208 | 86.00 |
| 9 | 110.8870 | 88.00 |
| 10 | 107.1212 | 85.60 |
| 11 | 96.0903 | 92.00 |
| 12 | 93.2902 | 91.60 |
| 13 | 97.6822 | 84.80 |
| 14 | 93.8540 | 86.40 |
| 15 | 83.4308 | 90.00 |
| 16 | 86.1369 | 87.60 |
| 17 | 83.6948 | 87.20 |
| 18 | 77.1734 | 89.20 |
| 19 | 76.8100 | 88.40 |
| 20 | 76.9154 | 88.40 |
| 21 | 72.9131 | 88.40 |
| 22 | 70.0470 | 88.80 |
| 23 | 71.1574 | 88.40 |
| 24 | 65.5507 | 90.40 |
| 25 | 64.1233 | 91.20 |
| 26 | 63.8426 | 91.20 |
| 27 | 59.0329 | 91.60 |
| 28 | 60.3236 | 91.20 |
| 29 | 57.8456 | 91.60 |
| 30 | 56.1021 | 91.60 |
| 31 | 55.7914 | 91.20 |
| 32 | 55.7293 | 91.20 |
| 33 | 55.1831 | 91.20 |
| 34 | 54.6307 | 91.20 |
| 35 | 51.7978 | 91.20 |
| 36 | 51.1896 | 91.20 |
| 37 | 52.3792 | 91.20 |
| 38 | 44.2613 | 92.40 |
| 39 | 47.1926 | 91.60 |
| 40 | 57.3296 | 89.20 |
| 41 | 46.6946 | 91.60 |
| 42 | 43.9059 | 92.40 |
| 43 | 49.3158 | 90.80 |
| 44 | 46.4154 | 91.60 |
| 45 | 47.1510 | 91.60 |
| 46 | 46.3668 | 91.60 |
| 47 | 40.7486 | 92.00 |
| 48 | 35.9420 | 93.60 |
| 49 | 43.9010 | 91.60 |
| 50 | 50.4984 | 90.80 |

## Dataset: `Spiral`
- Time per epoch: **0.0014** seconds
- Final loss: **162.6608**
- Final accuracy: **57.20%**

### Loss and Accuracy per Epoch

| Epoch | Loss | Accuracy (%) |
|-------|------|---------------|
| 1 | 170.2309 | 50.80 |
| 2 | 168.9634 | 55.20 |
| 3 | 168.0989 | 56.00 |
| 4 | 167.6678 | 56.40 |
| 5 | 167.5144 | 56.40 |
| 6 | 167.5260 | 55.60 |
| 7 | 167.3260 | 56.40 |
| 8 | 167.3930 | 56.40 |
| 9 | 167.1756 | 56.00 |
| 10 | 166.9901 | 56.80 |
| 11 | 166.8807 | 56.00 |
| 12 | 166.7795 | 56.40 |
| 13 | 166.7165 | 56.40 |
| 14 | 166.7572 | 56.00 |
| 15 | 166.5955 | 56.00 |
| 16 | 166.5240 | 55.60 |
| 17 | 166.3658 | 56.00 |
| 18 | 166.2058 | 56.00 |
| 19 | 166.1688 | 55.60 |
| 20 | 165.9918 | 55.60 |
| 21 | 165.9048 | 55.20 |
| 22 | 165.7574 | 55.60 |
| 23 | 165.6786 | 55.20 |
| 24 | 165.5063 | 55.20 |
| 25 | 165.3012 | 55.20 |
| 26 | 165.2006 | 55.20 |
| 27 | 165.1134 | 55.20 |
| 28 | 164.9487 | 55.20 |
| 29 | 164.8739 | 54.40 |
| 30 | 164.7236 | 54.40 |
| 31 | 164.6304 | 54.40 |
| 32 | 164.5391 | 54.00 |
| 33 | 164.3482 | 54.00 |
| 34 | 164.2994 | 53.20 |
| 35 | 164.0171 | 52.80 |
| 36 | 163.8546 | 52.40 |
| 37 | 163.5104 | 54.00 |
| 38 | 163.4351 | 52.80 |
| 39 | 163.5664 | 52.00 |
| 40 | 163.4861 | 52.40 |
| 41 | 163.5167 | 55.60 |
| 42 | 163.6317 | 57.20 |
| 43 | 163.5777 | 57.20 |
| 44 | 163.3293 | 57.20 |
| 45 | 163.0947 | 56.80 |
| 46 | 162.9979 | 57.20 |
| 47 | 162.8497 | 57.20 |
| 48 | 162.6405 | 56.80 |
| 49 | 162.9942 | 57.20 |
| 50 | 162.6608 | 57.20 |


---
---# Numba Parallel Diagnostics Report


## MAP

```

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_map.<locals>._map,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(178)
================================================================================


Parallel loop listing for  Function tensor_map.<locals>._map, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (178)
--------------------------------------------------------------------------------------|loop #ID
    def _map(                                                                         |
        out: Storage,                                                                 |
        out_shape: Shape,                                                             |
        out_strides: Strides,                                                         |
        in_storage: Storage,                                                          |
        in_shape: Shape,                                                              |
        in_strides: Strides,                                                          |
    ) -> None:                                                                        |
        same_shape = (out_shape == in_shape).all()------------------------------------| #0
        same_strides = (out_strides == in_strides).all()------------------------------| #1
                                                                                      |
        # Fast path: same shape and strides                                           |
        if same_shape and same_strides:                                               |
            for i in prange(len(out)):------------------------------------------------| #2
                out[i] = fn(in_storage[i])                                            |
            return                                                                    |
                                                                                      |
        # Slower path: need to compute index and broadcast                            |
        out_index = np.empty(len(out_shape), dtype=np.int32)                          |
        in_index = np.empty(len(in_shape), dtype=np.int32)                            |
                                                                                      |
        for i in prange(len(out)):----------------------------------------------------| #3
            # === Inline to_index(i, out_shape, out_index)                            |
            shape_strides = np.empty(len(out_shape), dtype=np.int32)                  |
            acc = 1                                                                   |
            for j in range(len(out_shape) - 1, -1, -1):                               |
                shape_strides[j] = acc                                                |
                acc *= out_shape[j]                                                   |
                                                                                      |
            for dim in range(len(out_shape)):                                         |
                out_index[dim] = (i // shape_strides[dim]) % out_shape[dim]           |
                                                                                      |
            # === Inline broadcast_index(out_index, out_shape, in_shape, in_index)    |
            in_offset = len(in_shape) - len(out_shape)                                |
            for j in range(len(in_shape)):                                            |
                if j < in_offset:                                                     |
                    in_index[j] = 0                                                   |
                else:                                                                 |
                    if in_shape[j] == 1:                                              |
                        in_index[j] = 0                                               |
                    else:                                                             |
                        in_index[j] = out_index[j - in_offset]                        |
                                                                                      |
            # === Inline index_to_position for in_index and out_index                 |
            in_pos = 0                                                                |
            for j in range(len(in_shape)):                                            |
                in_pos += in_index[j] * in_strides[j]                                 |
                                                                                      |
            out_pos = 0                                                               |
            for j in range(len(out_shape)):                                           |
                out_pos += out_index[j] * out_strides[j]                              |
                                                                                      |
            # === Apply function                                                      |
            out[out_pos] = fn(in_storage[in_pos])                                     |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 4 parallel for-
loop(s) (originating from loops labelled: #0, #1, #2, #3).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(201) is hoisted out of the parallel loop labelled #3 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: shape_strides = np.empty(len(out_shape), dtype=np.int32)
    - numpy.empty() is used for the allocation.

```

## ZIP

```

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_zip.<locals>._zip,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(249)
================================================================================


Parallel loop listing for  Function tensor_zip.<locals>._zip, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (249)
-----------------------------------------------------------------------------------|loop #ID
    def _zip(                                                                      |
        out: Storage,                                                              |
        out_shape: Shape,                                                          |
        out_strides: Strides,                                                      |
        a_storage: Storage,                                                        |
        a_shape: Shape,                                                            |
        a_strides: Strides,                                                        |
        b_storage: Storage,                                                        |
        b_shape: Shape,                                                            |
        b_strides: Strides,                                                        |
    ) -> None:                                                                     |
        # === Inline is_contiguous for all tensors                                 |
        def _is_contiguous(                                                        |
            shape: npt.NDArray[np.int32], strides: npt.NDArray[np.int32]           |
        ) -> bool:                                                                 |
            """Check whether the tensor's memory layout is contiguous.             |
                                                                                   |
            Returns                                                                |
            -------                                                                |
            bool                                                                   |
                True if memory layout is contiguous, False otherwise.              |
                                                                                   |
            """                                                                    |
            expected_stride = 1                                                    |
            for i in range(len(shape) - 1, -1, -1):                                |
                if shape[i] != 1 and strides[i] != expected_stride:                |
                    return False                                                   |
                expected_stride *= shape[i]                                        |
            return True                                                            |
                                                                                   |
        same_shape = len(out_shape) == len(a_shape) == len(b_shape)                |
        if same_shape:                                                             |
            same_shape = True                                                      |
            for i in range(len(out_shape)):                                        |
                if out_shape[i] != a_shape[i] or out_shape[i] != b_shape[i]:       |
                    same_shape = False                                             |
                    break                                                          |
                                                                                   |
        stride_aligned = (                                                         |
            _is_contiguous(out_shape, out_strides)                                 |
            and _is_contiguous(a_shape, a_strides)                                 |
            and _is_contiguous(b_shape, b_strides)                                 |
            and same_shape                                                         |
        )                                                                          |
                                                                                   |
        if stride_aligned:                                                         |
            for i in prange(len(out)):---------------------------------------------| #4
                out[i] = fn(a_storage[i], b_storage[i])                            |
        else:                                                                      |
            out_index = np.empty(len(out_shape), dtype=np.int32)                   |
            a_index = np.empty(len(a_shape), dtype=np.int32)                       |
            b_index = np.empty(len(b_shape), dtype=np.int32)                       |
            shape_strides = np.empty(len(out_shape), dtype=np.int32)               |
                                                                                   |
            acc = 1                                                                |
            for j in range(len(out_shape) - 1, -1, -1):                            |
                shape_strides[j] = acc                                             |
                acc *= out_shape[j]                                                |
                                                                                   |
            for i in prange(len(out)):---------------------------------------------| #5
                for dim in range(len(out_shape)):                                  |
                    out_index[dim] = (i // shape_strides[dim]) % out_shape[dim]    |
                                                                                   |
                a_offset = len(a_shape) - len(out_shape)                           |
                for j in range(len(a_shape)):                                      |
                    if j < a_offset or a_shape[j] == 1:                            |
                        a_index[j] = 0                                             |
                    else:                                                          |
                        a_index[j] = out_index[j - a_offset]                       |
                                                                                   |
                b_offset = len(b_shape) - len(out_shape)                           |
                for j in range(len(b_shape)):                                      |
                    if j < b_offset or b_shape[j] == 1:                            |
                        b_index[j] = 0                                             |
                    else:                                                          |
                        b_index[j] = out_index[j - b_offset]                       |
                                                                                   |
                a_pos = 0                                                          |
                for j in range(len(a_shape)):                                      |
                    a_pos += a_index[j] * a_strides[j]                             |
                                                                                   |
                b_pos = 0                                                          |
                for j in range(len(b_shape)):                                      |
                    b_pos += b_index[j] * b_strides[j]                             |
                                                                                   |
                out_pos = 0                                                        |
                for j in range(len(out_shape)):                                    |
                    out_pos += out_index[j] * out_strides[j]                       |
                                                                                   |
                out[out_pos] = fn(a_storage[a_pos], b_storage[b_pos])              |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 2 parallel for-
loop(s) (originating from loops labelled: #4, #5).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found

```

## REDUCE

```

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_reduce.<locals>._reduce,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(364)
================================================================================


Parallel loop listing for  Function tensor_reduce.<locals>._reduce, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (364)
------------------------------------------------------------------------------------|loop #ID
    def _reduce(                                                                    |
        out: Storage,                                                               |
        out_shape: Shape,                                                           |
        out_strides: Strides,                                                       |
        a_storage: Storage,                                                         |
        a_shape: Shape,                                                             |
        a_strides: Strides,                                                         |
        reduce_dim: int,                                                            |
    ) -> None:                                                                      |
        out_index = np.empty(len(out_shape), dtype=np.int32)                        |
        a_index = np.empty(len(a_shape), dtype=np.int32)                            |
        shape_strides = np.empty(len(out_shape), dtype=np.int32)                    |
                                                                                    |
        # Precompute shape strides for to_index                                     |
        acc = 1                                                                     |
        for j in range(len(out_shape) - 1, -1, -1):                                 |
            shape_strides[j] = acc                                                  |
            acc *= out_shape[j]                                                     |
                                                                                    |
        reduce_size = a_shape[reduce_dim]                                           |
                                                                                    |
        for i in prange(len(out)):--------------------------------------------------| #6
            # === Inline to_index(i, out_shape, out_index)                          |
            for j in range(len(out_shape)):                                         |
                out_index[j] = (i // shape_strides[j]) % out_shape[j]               |
                                                                                    |
            # === Copy out_index to a_index (broadcasting not needed for reduce)    |
            for j in range(len(out_shape)):                                         |
                a_index[j] = out_index[j]                                           |
                                                                                    |
            # === First element of reduction                                        |
            a_index[reduce_dim] = 0                                                 |
            a_pos = 0                                                               |
            for j in range(len(a_shape)):                                           |
                a_pos += a_index[j] * a_strides[j]                                  |
            acc = a_storage[a_pos]                                                  |
                                                                                    |
            # === Accumulate over reduce dimension                                  |
            for j in range(1, reduce_size):                                         |
                a_index[reduce_dim] = j                                             |
                a_pos = 0                                                           |
                for k in range(len(a_shape)):                                       |
                    a_pos += a_index[k] * a_strides[k]                              |
                acc = fn(acc, a_storage[a_pos])                                     |
                                                                                    |
            # === Compute output position                                           |
            out_pos = 0                                                             |
            for j in range(len(out_shape)):                                         |
                out_pos += out_index[j] * out_strides[j]                            |
                                                                                    |
            out[out_pos] = acc                                                      |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #6).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found

```

## MATRIX MULTIPLY

```

================================================================================
 Parallel Accelerator Optimizing:  Function _tensor_matrix_multiply,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(419)
================================================================================


Parallel loop listing for  Function _tensor_matrix_multiply, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (419)
---------------------------------------------------------------------------------------|loop #ID
def _tensor_matrix_multiply(                                                           |
    out: Storage,                                                                      |
    out_shape: Shape,                                                                  |
    out_strides: Strides,                                                              |
    a_storage: Storage,                                                                |
    a_shape: Shape,                                                                    |
    a_strides: Strides,                                                                |
    b_storage: Storage,                                                                |
    b_shape: Shape,                                                                    |
    b_strides: Strides,                                                                |
) -> None:                                                                             |
    """NUMBA tensor matrix multiply function.                                          |
                                                                                       |
    Should work for any tensor shapes that broadcast as long as                        |
                                                                                       |
    ```                                                                                |
    assert a_shape[-1] == b_shape[-2]                                                  |
    ```                                                                                |
                                                                                       |
    Optimizations:                                                                     |
                                                                                       |
    * Outer loop in parallel                                                           |
    * No index buffers or function calls                                               |
    * Inner loop should have no global writes, 1 multiply.                             |
                                                                                       |
                                                                                       |
    Args:                                                                              |
    ----                                                                               |
        out (Storage): storage for `out` tensor                                        |
        out_shape (Shape): shape for `out` tensor                                      |
        out_strides (Strides): strides for `out` tensor                                |
        a_storage (Storage): storage for `a` tensor                                    |
        a_shape (Shape): shape for `a` tensor                                          |
        a_strides (Strides): strides for `a` tensor                                    |
        b_storage (Storage): storage for `b` tensor                                    |
        b_shape (Shape): shape for `b` tensor                                          |
        b_strides (Strides): strides for `b` tensor                                    |
                                                                                       |
    Returns:                                                                           |
    -------                                                                            |
        None : Fills in `out`                                                          |
                                                                                       |
    """                                                                                |
    # Ensure inner dimensions align for matrix multiplication                          |
    assert a_shape[-1] == b_shape[-2]                                                  |
                                                                                       |
    # Extract dimensions                                                               |
    batch = out_shape[0]  # After reshaping, tensors are always 3D                     |
    out_i = out_shape[1]                                                               |
    out_j = out_shape[2]                                                               |
    inner_dim = a_shape[2]  # Shared dimension for dot product                         |
                                                                                       |
    # Parallelize over batch dimension                                                 |
    for n in prange(batch):------------------------------------------------------------| #7
        for i in range(out_i):                                                         |
            for j in range(out_j):                                                     |
                acc = 0.0                                                              |
                for k in range(inner_dim):                                             |
                    # Handle broadcasting: if shape is 1 in batch, reuse 0-th index    |
                    a_n = n if a_shape[0] > 1 else 0                                   |
                    b_n = n if b_shape[0] > 1 else 0                                   |
                                                                                       |
                    # Compute flat index into a[n, i, k]                               |
                    a_pos = int(                                                       |
                        a_n * a_strides[0] +                                           |
                        i * a_strides[1] +                                             |
                        k * a_strides[2]                                               |
                    )                                                                  |
                                                                                       |
                    # Compute flat index into b[n, k, j]                               |
                    b_pos = int(                                                       |
                        b_n * b_strides[0] +                                           |
                        k * b_strides[1] +                                             |
                        j * b_strides[2]                                               |
                    )                                                                  |
                                                                                       |
                    acc += a_storage[a_pos] * b_storage[b_pos]                         |
                                                                                       |
                # Compute flat index into out[n, i, j]                                 |
                out_pos = int(                                                         |
                    n * out_strides[0] +                                               |
                    i * out_strides[1] +                                               |
                    j * out_strides[2]                                                 |
                )                                                                      |
                                                                                       |
                out[out_pos] = acc                                                     |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #7).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found

```
# Numba Parallel Diagnostics Report


## MAP

```

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_map.<locals>._map,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(178)
================================================================================


Parallel loop listing for  Function tensor_map.<locals>._map, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (178)
--------------------------------------------------------------------------------------|loop #ID
    def _map(                                                                         |
        out: Storage,                                                                 |
        out_shape: Shape,                                                             |
        out_strides: Strides,                                                         |
        in_storage: Storage,                                                          |
        in_shape: Shape,                                                              |
        in_strides: Strides,                                                          |
    ) -> None:                                                                        |
        same_shape = (out_shape == in_shape).all()------------------------------------| #0
        same_strides = (out_strides == in_strides).all()------------------------------| #1
                                                                                      |
        # Fast path: same shape and strides                                           |
        if same_shape and same_strides:                                               |
            for i in prange(len(out)):------------------------------------------------| #2
                out[i] = fn(in_storage[i])                                            |
            return                                                                    |
                                                                                      |
        # Slower path: need to compute index and broadcast                            |
        out_index = np.empty(len(out_shape), dtype=np.int32)                          |
        in_index = np.empty(len(in_shape), dtype=np.int32)                            |
                                                                                      |
        for i in prange(len(out)):----------------------------------------------------| #3
            # === Inline to_index(i, out_shape, out_index)                            |
            shape_strides = np.empty(len(out_shape), dtype=np.int32)                  |
            acc = 1                                                                   |
            for j in range(len(out_shape) - 1, -1, -1):                               |
                shape_strides[j] = acc                                                |
                acc *= out_shape[j]                                                   |
                                                                                      |
            for dim in range(len(out_shape)):                                         |
                out_index[dim] = (i // shape_strides[dim]) % out_shape[dim]           |
                                                                                      |
            # === Inline broadcast_index(out_index, out_shape, in_shape, in_index)    |
            in_offset = len(in_shape) - len(out_shape)                                |
            for j in range(len(in_shape)):                                            |
                if j < in_offset:                                                     |
                    in_index[j] = 0                                                   |
                else:                                                                 |
                    if in_shape[j] == 1:                                              |
                        in_index[j] = 0                                               |
                    else:                                                             |
                        in_index[j] = out_index[j - in_offset]                        |
                                                                                      |
            # === Inline index_to_position for in_index and out_index                 |
            in_pos = 0                                                                |
            for j in range(len(in_shape)):                                            |
                in_pos += in_index[j] * in_strides[j]                                 |
                                                                                      |
            out_pos = 0                                                               |
            for j in range(len(out_shape)):                                           |
                out_pos += out_index[j] * out_strides[j]                              |
                                                                                      |
            # === Apply function                                                      |
            out[out_pos] = fn(in_storage[in_pos])                                     |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 4 parallel for-
loop(s) (originating from loops labelled: #0, #1, #2, #3).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(201) is hoisted out of the parallel loop labelled #3 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: shape_strides = np.empty(len(out_shape), dtype=np.int32)
    - numpy.empty() is used for the allocation.

```

## ZIP

```

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_zip.<locals>._zip,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(249)
================================================================================


Parallel loop listing for  Function tensor_zip.<locals>._zip, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (249)
-----------------------------------------------------------------------------------|loop #ID
    def _zip(                                                                      |
        out: Storage,                                                              |
        out_shape: Shape,                                                          |
        out_strides: Strides,                                                      |
        a_storage: Storage,                                                        |
        a_shape: Shape,                                                            |
        a_strides: Strides,                                                        |
        b_storage: Storage,                                                        |
        b_shape: Shape,                                                            |
        b_strides: Strides,                                                        |
    ) -> None:                                                                     |
        # === Inline is_contiguous for all tensors                                 |
        def _is_contiguous(                                                        |
            shape: npt.NDArray[np.int32], strides: npt.NDArray[np.int32]           |
        ) -> bool:                                                                 |
            """Check whether the tensor's memory layout is contiguous.             |
                                                                                   |
            Returns                                                                |
            -------                                                                |
            bool                                                                   |
                True if memory layout is contiguous, False otherwise.              |
                                                                                   |
            """                                                                    |
            expected_stride = 1                                                    |
            for i in range(len(shape) - 1, -1, -1):                                |
                if shape[i] != 1 and strides[i] != expected_stride:                |
                    return False                                                   |
                expected_stride *= shape[i]                                        |
            return True                                                            |
                                                                                   |
        same_shape = len(out_shape) == len(a_shape) == len(b_shape)                |
        if same_shape:                                                             |
            same_shape = True                                                      |
            for i in range(len(out_shape)):                                        |
                if out_shape[i] != a_shape[i] or out_shape[i] != b_shape[i]:       |
                    same_shape = False                                             |
                    break                                                          |
                                                                                   |
        stride_aligned = (                                                         |
            _is_contiguous(out_shape, out_strides)                                 |
            and _is_contiguous(a_shape, a_strides)                                 |
            and _is_contiguous(b_shape, b_strides)                                 |
            and same_shape                                                         |
        )                                                                          |
                                                                                   |
        if stride_aligned:                                                         |
            for i in prange(len(out)):---------------------------------------------| #4
                out[i] = fn(a_storage[i], b_storage[i])                            |
        else:                                                                      |
            out_index = np.empty(len(out_shape), dtype=np.int32)                   |
            a_index = np.empty(len(a_shape), dtype=np.int32)                       |
            b_index = np.empty(len(b_shape), dtype=np.int32)                       |
            shape_strides = np.empty(len(out_shape), dtype=np.int32)               |
                                                                                   |
            acc = 1                                                                |
            for j in range(len(out_shape) - 1, -1, -1):                            |
                shape_strides[j] = acc                                             |
                acc *= out_shape[j]                                                |
                                                                                   |
            for i in prange(len(out)):---------------------------------------------| #5
                for dim in range(len(out_shape)):                                  |
                    out_index[dim] = (i // shape_strides[dim]) % out_shape[dim]    |
                                                                                   |
                a_offset = len(a_shape) - len(out_shape)                           |
                for j in range(len(a_shape)):                                      |
                    if j < a_offset or a_shape[j] == 1:                            |
                        a_index[j] = 0                                             |
                    else:                                                          |
                        a_index[j] = out_index[j - a_offset]                       |
                                                                                   |
                b_offset = len(b_shape) - len(out_shape)                           |
                for j in range(len(b_shape)):                                      |
                    if j < b_offset or b_shape[j] == 1:                            |
                        b_index[j] = 0                                             |
                    else:                                                          |
                        b_index[j] = out_index[j - b_offset]                       |
                                                                                   |
                a_pos = 0                                                          |
                for j in range(len(a_shape)):                                      |
                    a_pos += a_index[j] * a_strides[j]                             |
                                                                                   |
                b_pos = 0                                                          |
                for j in range(len(b_shape)):                                      |
                    b_pos += b_index[j] * b_strides[j]                             |
                                                                                   |
                out_pos = 0                                                        |
                for j in range(len(out_shape)):                                    |
                    out_pos += out_index[j] * out_strides[j]                       |
                                                                                   |
                out[out_pos] = fn(a_storage[a_pos], b_storage[b_pos])              |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 2 parallel for-
loop(s) (originating from loops labelled: #4, #5).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found

```

## REDUCE

```

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_reduce.<locals>._reduce,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(364)
================================================================================


Parallel loop listing for  Function tensor_reduce.<locals>._reduce, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (364)
------------------------------------------------------------------------------------|loop #ID
    def _reduce(                                                                    |
        out: Storage,                                                               |
        out_shape: Shape,                                                           |
        out_strides: Strides,                                                       |
        a_storage: Storage,                                                         |
        a_shape: Shape,                                                             |
        a_strides: Strides,                                                         |
        reduce_dim: int,                                                            |
    ) -> None:                                                                      |
        out_index = np.empty(len(out_shape), dtype=np.int32)                        |
        a_index = np.empty(len(a_shape), dtype=np.int32)                            |
        shape_strides = np.empty(len(out_shape), dtype=np.int32)                    |
                                                                                    |
        # Precompute shape strides for to_index                                     |
        acc = 1                                                                     |
        for j in range(len(out_shape) - 1, -1, -1):                                 |
            shape_strides[j] = acc                                                  |
            acc *= out_shape[j]                                                     |
                                                                                    |
        reduce_size = a_shape[reduce_dim]                                           |
                                                                                    |
        for i in prange(len(out)):--------------------------------------------------| #6
            # === Inline to_index(i, out_shape, out_index)                          |
            for j in range(len(out_shape)):                                         |
                out_index[j] = (i // shape_strides[j]) % out_shape[j]               |
                                                                                    |
            # === Copy out_index to a_index (broadcasting not needed for reduce)    |
            for j in range(len(out_shape)):                                         |
                a_index[j] = out_index[j]                                           |
                                                                                    |
            # === First element of reduction                                        |
            a_index[reduce_dim] = 0                                                 |
            a_pos = 0                                                               |
            for j in range(len(a_shape)):                                           |
                a_pos += a_index[j] * a_strides[j]                                  |
            acc = a_storage[a_pos]                                                  |
                                                                                    |
            # === Accumulate over reduce dimension                                  |
            for j in range(1, reduce_size):                                         |
                a_index[reduce_dim] = j                                             |
                a_pos = 0                                                           |
                for k in range(len(a_shape)):                                       |
                    a_pos += a_index[k] * a_strides[k]                              |
                acc = fn(acc, a_storage[a_pos])                                     |
                                                                                    |
            # === Compute output position                                           |
            out_pos = 0                                                             |
            for j in range(len(out_shape)):                                         |
                out_pos += out_index[j] * out_strides[j]                            |
                                                                                    |
            out[out_pos] = acc                                                      |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #6).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found

```

## MATRIX MULTIPLY

```

================================================================================
 Parallel Accelerator Optimizing:  Function _tensor_matrix_multiply,
C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py
(419)
================================================================================


Parallel loop listing for  Function _tensor_matrix_multiply, C:\Users\Ouissal\workspace-minitorch\minitorch-wstoloah\minitorch\fast_ops.py (419)
---------------------------------------------------------------------------------------|loop #ID
def _tensor_matrix_multiply(                                                           |
    out: Storage,                                                                      |
    out_shape: Shape,                                                                  |
    out_strides: Strides,                                                              |
    a_storage: Storage,                                                                |
    a_shape: Shape,                                                                    |
    a_strides: Strides,                                                                |
    b_storage: Storage,                                                                |
    b_shape: Shape,                                                                    |
    b_strides: Strides,                                                                |
) -> None:                                                                             |
    """NUMBA tensor matrix multiply function.                                          |
                                                                                       |
    Should work for any tensor shapes that broadcast as long as                        |
                                                                                       |
    ```                                                                                |
    assert a_shape[-1] == b_shape[-2]                                                  |
    ```                                                                                |
                                                                                       |
    Optimizations:                                                                     |
                                                                                       |
    * Outer loop in parallel                                                           |
    * No index buffers or function calls                                               |
    * Inner loop should have no global writes, 1 multiply.                             |
                                                                                       |
                                                                                       |
    Args:                                                                              |
    ----                                                                               |
        out (Storage): storage for `out` tensor                                        |
        out_shape (Shape): shape for `out` tensor                                      |
        out_strides (Strides): strides for `out` tensor                                |
        a_storage (Storage): storage for `a` tensor                                    |
        a_shape (Shape): shape for `a` tensor                                          |
        a_strides (Strides): strides for `a` tensor                                    |
        b_storage (Storage): storage for `b` tensor                                    |
        b_shape (Shape): shape for `b` tensor                                          |
        b_strides (Strides): strides for `b` tensor                                    |
                                                                                       |
    Returns:                                                                           |
    -------                                                                            |
        None : Fills in `out`                                                          |
                                                                                       |
    """                                                                                |
    # Ensure inner dimensions align for matrix multiplication                          |
    assert a_shape[-1] == b_shape[-2]                                                  |
                                                                                       |
    # Extract dimensions                                                               |
    batch = out_shape[0]  # After reshaping, tensors are always 3D                     |
    out_i = out_shape[1]                                                               |
    out_j = out_shape[2]                                                               |
    inner_dim = a_shape[2]  # Shared dimension for dot product                         |
                                                                                       |
    # Parallelize over batch dimension                                                 |
    for n in prange(batch):------------------------------------------------------------| #7
        for i in range(out_i):                                                         |
            for j in range(out_j):                                                     |
                acc = 0.0                                                              |
                for k in range(inner_dim):                                             |
                    # Handle broadcasting: if shape is 1 in batch, reuse 0-th index    |
                    a_n = n if a_shape[0] > 1 else 0                                   |
                    b_n = n if b_shape[0] > 1 else 0                                   |
                                                                                       |
                    # Compute flat index into a[n, i, k]                               |
                    a_pos = int(                                                       |
                        a_n * a_strides[0] +                                           |
                        i * a_strides[1] +                                             |
                        k * a_strides[2]                                               |
                    )                                                                  |
                                                                                       |
                    # Compute flat index into b[n, k, j]                               |
                    b_pos = int(                                                       |
                        b_n * b_strides[0] +                                           |
                        k * b_strides[1] +                                             |
                        j * b_strides[2]                                               |
                    )                                                                  |
                                                                                       |
                    acc += a_storage[a_pos] * b_storage[b_pos]                         |
                                                                                       |
                # Compute flat index into out[n, i, j]                                 |
                out_pos = int(                                                         |
                    n * out_strides[0] +                                               |
                    i * out_strides[1] +                                               |
                    j * out_strides[2]                                                 |
                )                                                                      |
                                                                                       |
                out[out_pos] = acc                                                     |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #7).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found

```
